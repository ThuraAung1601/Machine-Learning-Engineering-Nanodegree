{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker\n",
    "\n",
    "- What is it? \n",
    " - Managed jupyter workbook instance\n",
    " - Runs on virtual machine hosted by Amazon\n",
    " - Application Programming Interface (API)\n",
    "\n",
    "- Why use it?\n",
    " - Simplifies ML Workflow. Notebook can be used for step 1 (Explore & Process Data), whereas the API helps in steps 2 and 3 (Modeling, Deployment)\n",
    "\n",
    "- How does it work?\n",
    " - Working in managed workbook\n",
    " - API: Toolkit\n",
    " \n",
    " \n",
    "## AWS Educate\n",
    "\n",
    "- [Cloud Career Pathways](https://www.awseducate.com/student/s/pathways?sc_content=Other_ln_ot&sc_outcome=other&sc_medium=em_1218&mkt_tok=eyJpIjoiWTJGaFlqUTFOelJqTVRrMyIsInQiOiJ2ZDVUenljY2VaVHV5Z0RIenZ2Mkg3eFE3WEVvM3g5TFVEWU1nZXR5bkplZDhLZHlNK21TOUgyWWxcL3FNU2xXbjYxTVRFUlI4SzdjdXZiQUlRNWNhbmNjaWlFb2ZpakJYaEV0YzR3Q01GcFp4dUtsMWJtZ3pnbVA4ZGJVR3g3d1QifQ%3D%3D&sc_channel=em&sc_detail=awseducatewelcomeemail1&sc_country=mult&trk=em_1218&sc_campaign=GLOBAL_LN_aws-educate-welcome-series_20180312&sc_geo=mult)\n",
    "- [Video walk-through](https://youtu.be/X36oOsjyoXA)\n",
    "- [AWS Training](https://www.aws.training/)\n",
    "- [Getting Started](https://aws.amazon.com/de/getting-started/?sc_channel=em&sc_campaign=global_F90D_DF_F7D_E1_GetStarted_2017&sc_medium=em_50315&sc_content=adopt_f90d_f90d_ot&sc_geo=mult&sc_country=global&sc_outcome=adopt_f90d&trk=em_50315&mkt_tok=eyJpIjoiWldNME4yUTRaall6WkRRMSIsInQiOiJtQ0w3RFBKOEdRZ252RkZVTWtIS3ZVY1JmRytvWUVkNmc5UDRvTmFTcGZPcFVEQ29hZDUxd2pvMVR0VVRMcmR3K0x3Rkp4b3lcL1pvcjJNcDRLaVlhT1wvS2dva0NuQkpjb0RJZ1wvTjhKK3Q4TGdHR0RLWENNVXo0anBzNDlhZnBiQjlXZ1VaTzNmNUtSZE0zbVdZN3NOZ0E9PSJ9)\n",
    "\n",
    "#### 4 Ways to get started\n",
    "\n",
    "1. Start Building a Solution: [Build a Web App](https://email.awscloud.com/dc/2mk777umeRArLOQYC8WPqn_vFW-YQzHA81_9GObtUnZ3rj0CpnSoLJdQXLaAMz00lo0js8sxJ1Z9sEkM731nPIKXjNmPNptU1XJXb400xm2FUMbhw9FtaJnRd_NqAAko7aBJUyuP5bVStNHCaqwq3eJe7kS4Yu7zrlI7ZdlEf0u1bzxrqrnBSn4A1Bc7a6YVTv2D1KY97lD1VhwpSZ31MIWGPExRVrtL20sqM-VLxiyf0SyavAq9sHz2G4piyon5l4s4GGbQtlLOrwayTk_BndjMDJ92kacr6G6L3Pb3Tgormt7hl9lzta4xKWTjnlBflJrq4soNGZBHYxx8nEeH1tbSUNbuZNYKnyyabukrv-ZDDQzYD7WfpoA1CRmUO7JRJcETBxAG3hIw6tM7oNc5ng==/mj00kU00EdItSOE3ZSm0TM0), [Register a Domain](https://email.awscloud.com/dc/yuZkfPEXXFT65tdReNucEgQufV9pSoK35GaL7Qe8rgKw3LXT9k8odsOTsX6OvAGUB6AaAVT2f9dA1JoLLv42BT5_LSERNTIuB1_9dQLyBAQ6vh9w_2QTVBcrR9nhr5Hdhi22HqA2YLonM-l0YzbNVMy70xI5HyN00dsFkCf0KFbBnVWYAk9Rjt-1XUV7v-pGmvMpGANSYc2BQmBNrJyySOBoAKKA10abFrtns2285nayJ3Bm8wg77tbm4RIMfApo3XnhgjIHjaSVYRArZEPbA4CcyIbYXhR3Nql6WPWaATLM9uMBAK5-xPJJZhc2syAkAqx4-s8r7wygssYRGE5gJqsthyELdyDBqN2S8ll1R9E=/mj00kU00EdItSOE3ZSm0TM0), or [Start A Development Project](https://email.awscloud.com/dc/2mk777umeRArLOQYC8WPqn_vFW-YQzHA81_9GObtUnY_D0cOOuSkjHwN3t1RbSNp4-pEeZveR6d9gAbq0KWWaohH_1RidMbIAeCGehWGAOOzhi4jS2GXpySs3ygBADTaXWXXzqBvTDpXGIP-Gy5i6y71gDC8o_2sl4xaIEQqzY8l9IazFuqGKg79Y7DAGbTmMR8MFrd1j9wNo_LOKk7OaD-PPjUkPiAerjFd92oQfDCiPW-cS5hh5Nvy2LLDF3D9JHkwMooAisTK0D4n7is2MaUssOYBpbYod3uU2qY8DvgQNSzNGc2WLUtELvtbwjEntnClufR4cmypcBo-7N_Eq0tjTdBe0QjAgpYVIgi0zOxfJ6JOVhBLh43K9m8U95fk/mj00kU00EdItSOE3ZSm0TM0)\n",
    "2. Try a 10-Minute Tutorial: [View all 20+ Tutorials](https://email.awscloud.com/dc/KwqiTCOQ16Q1JCi3MdelD3ZZGcncVANDEUTkhckMhmiIJfxnxf4lwMKZXcrCCTTJhLqgAL0dTPTUP351riF3Jcqyf0EuHmFX0s88vt7mO887gRmgOacPdK84xaaq_SI_UYrNo605JTPwuF655LllDfVwcFKSTII2GLhcZXseTPWDIiu_cBRtuyD88mbhnVwAHUj6Xi5m6DS8qrw_tZA7pj8M9WiNS5o8A27ekJR9niYg32fnpq9COrB7z5osoE_LLFXYaO8CmPm7w0az5Yoj_6aHzAOGshi14CT5Fgx7s_4esR3d2in3ngJMo4Evjl2uuFjnWoAUiM-PukeYRKeJMA==/mj00kU00EdItSOE3ZSm0TM0)\n",
    "3. Explore Getting Started Resources: [Explore the Resource Center](https://email.awscloud.com/dc/KwqiTCOQ16Q1JCi3MdelD3ZZGcncVANDEUTkhckMhmjKLpc5CeluLOBeryeJSliZOd7HSaDnjFzd0Abmoc1XBrj18JVubOZatd9mgWiLqqGUbPjDlpEp_NUp34Lja7nw44kUpbdaiasloBbQCPD1ganh3ViQqrN9GSGEfUqpSpJQhEJe4zNXFrBt6BRKZGx70-xt49WuzhqBHq4iKgy9xakTavlJEDPM2Zs5J8ofy909Ig4j_NEqrQXgLv6YNxrcXalzSMqPTGH8Ufw6r9_7J8skK--J3rUxIvp_CG1vXW1BQ2FYa42JWZHQKFga5hpy9KjRIknXO9OwOrx3eSmMPQ==/mj00kU00EdItSOE3ZSm0TM0)\n",
    "4. Try Aamzon's Simple Cloud Server: [Learn more about Ligthtsail](https://email.awscloud.com/dc/KwqiTCOQ16Q1JCi3MdelD7X2Wojh-YozQqZVBYS1snIomUbGOOzVOcJ_Q0iLJPEC-B-oRMFime8Yha-t7yeS7YeHC0rwIplN8YglIjArI7zQgl6UL3wHxJzP2kJeOnE3w7-3z-HUG5gBPPM4KUKN-oglpLT_Wc3bbc4pT52geRoFNdFZX4nZVomtCzEn_m2Rqoeemw3jiWFJV7owVfodN6JyAXzDyI_5QVzZTYAcy5dQc8EJJh0ydhO7lxwbkA0Zoa001aLOQZNOrJax5O8LoK8Fy7_YZK9pAgz8PHe5mWY8Qg8LK3gdun5ypptUuw-GCaJIu0ulV5fWERn7ruUfJA==/mj00kU00EdItSOE3ZSm0TM0)\n",
    "\n",
    "Further resources: \n",
    "\n",
    "- Dataset that will be used: [Boston Housing Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)\n",
    "- Documentation for the high level API can be found [here](http://sagemaker.readthedocs.io/en/latest/)\n",
    "- Tutorial: [Build, Train, and Deploy a Machine Learning Model](https://aws.amazon.com/de/getting-started/tutorials/build-train-deploy-machine-learning-model-sagemaker/?trk=gs_card) with Amazon SageMaker\n",
    "- [What is Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Access\n",
    "\n",
    "- Smallest GPU instance available when using SageMaker is the **ml.p2.xlarge**\n",
    " - Adequate for this project. \n",
    " - [x] Request limit increase [here](https://console.aws.amazon.com/ec2/v2/home?#Limits)\n",
    " \n",
    "#### Setting up a Notebook instance\n",
    "\n",
    "- Interaction with SageMaker ecosystem using notebook instances\n",
    "- Go to console and open the \"SageMaker\" service. \n",
    "- \"Notebook\" > click \"Notebook instances\"\n",
    "- \"Create Notebook instance\"\n",
    "- Name the notebook instance, \n",
    " - notebook instance type: \"ml.t2.medium\" should be all that is necessary for the notebooks for now. IN addition, the are covered under the free tier (check what's covered).\n",
    "- IAM role: (requires access to \"S3 bucket with \"sagemaker\" in the name\")\n",
    " - S3 buckets > None (no additional buckets)\n",
    " - S3 can be thought of as place to store data (e.g. training data)\n",
    " - click \"create role\"\n",
    "- Create notebook instance and wait till Status changes\n",
    "\n",
    "#### Cloning the Deployment Notebooks\n",
    "\n",
    "- SageMaker can be linked directly to a Github repo and it's recommended to do so\n",
    "\n",
    "```\n",
    "# if not specified in git, go to Terminal\n",
    "cd SageMaker\n",
    "git clone http://github.com/udacity/sagemaker-deployment.git\n",
    "exit\n",
    "```\n",
    "\n",
    "- Also, on the `Actions` list, you should select `Open Jupyter` to get the examples notebooks\n",
    "\n",
    "#### SageMaker Sessions & Execution Roles\n",
    "\n",
    "SageMaker has some unique objects and terminology. There are a few objects that you'll see come up, over and over again: \n",
    "\n",
    "- **Session** - A session is a special object that allows things like manage data in S3 and create and train any machine learning models. \n",
    " - More information [here](https://sagemaker.readthedocs.io/en/latest/session.html). \n",
    " - `upload_data` function should be close to the top of the list. \n",
    " - Other functions, like `train`, `tune`, and `create_model` will be covered later\n",
    "- **Role** (also called execution role) - IAM role that you created. Defines how data that your notebook uses/creates will be stored. You can print out the role with `print(role)` to see the details of this creation.\n",
    "\n",
    "#### Uploading to an S3 Bucket\n",
    "\n",
    "- **S3** - Simple storage service (S3)\n",
    "- Method of data storage\n",
    "\n",
    "> S3 is a virtual storage solution that is mostly meant for data to be written to few times and read from many times. This is, in some sense, the main workhorse for data storage and transfer when using Amazon services. These are similar to file folders that contain data and metadata about that data, such as the data size, date of upload, author, and so on.\n",
    "\n",
    "- If data are uploaded to a session, then an S3 bucket is created\n",
    "\n",
    "This is indicated by something like the following note: \n",
    "\n",
    "```\n",
    "INFO: sagemaker: Created S3 bucket: <message specific to your locatle, ex. sagemaker-use-west-1#> \n",
    "```\n",
    "\n",
    "- More about how creating a csv file in the [pandas docs](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html)\n",
    "- In the example: Concatenating x and y data sets as columns of data (`axis=1`) and converting that pandas dataframe into a csv file using `.to_csv`. \n",
    "\n",
    "\n",
    "## SageMaker Example 1: Boston Housing\n",
    "\n",
    "- Goal: Predict the median value of a home in Boston Mass. \n",
    " - Data: [Boston Housing Data](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)\n",
    "\n",
    "\n",
    "### Step 0: Setting up the notebook\n",
    "\n",
    "- Importing all the required standard libraries:\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# vis\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import data from sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "import sklearn.model_selection\n",
    "```\n",
    "- Import SageMaker-specific libraries\n",
    "\n",
    "```python \n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "```\n",
    "\n",
    "- `sagemaker.Session()` - This is an object that represents the SageMaker session that we are currently operating in. This object contains some useful information that we will need to access later such as our region.\n",
    "- `get_execution_role()` - This is an object that represents the IAM role that we are currently assigned. When we construct and launch the training job later we will need to tell it what IAM role it should have. Since our use case is relatively simple we will simply assign the training job the role we currently have.\n",
    "\n",
    "### Step 1: Downloading the data\n",
    "\n",
    "- Loading data with sklearn\n",
    "\n",
    "```python\n",
    "boston = load_boston()\n",
    "print(\"Shape Boston data:\", boston.data.shape)\n",
    "```\n",
    "\n",
    "### Step 2: Preparing and splitting the data\n",
    "\n",
    "- Data is clean and does not need any processing\n",
    "- `sklearn.model_selection.train_test_split` can be used to do this ([link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "- The dataset's X and Y variables can be extracted using: \n",
    "\n",
    "```python\n",
    "X_bos_pd = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "Y_bos_pd = pd.DataFrame(boston.target)\n",
    "```\n",
    "\n",
    "- Splitting the data\n",
    "\n",
    "```python \n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X_bos_pd, Y_bos_pd, test_size=0.33)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(X_train, Y_train, test_size=0.33)\n",
    "```\n",
    "\n",
    "\n",
    "### Step 3: Uploading the data files to S3\n",
    "\n",
    "- Training job construction using SageMaker executes a container\n",
    "- The container performs the training operation. \n",
    "- This container is given access to data that is stored in S3. \n",
    " - Hence, we need to upload the data we want to use for training to S3.\n",
    "- Performing batch transform job: SageMaker expects the input data to be stored in S3 \n",
    "\n",
    "We can use the SageMaker API to achieve all this and hide some of the details. \n",
    "\n",
    "\n",
    "#### Step 3.1: Save the data locally\n",
    "\n",
    "- Save data as csv files in a local folder\n",
    "- Then, upload data to S3.\n",
    "\n",
    "```python\n",
    "# make sure local data directory exists\n",
    "data_dir = '../data/boston'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "```\n",
    "\n",
    "- We need to create test, train and validation as csv files which will then be uploaded to S3. \n",
    "- We use pandas to do this. Do not include header, information or an index\n",
    "- For the train and validation data, it is assumed that the first entry in each row is the target variable.\n",
    "- Q: What does `pd.concat` do?\n",
    "\n",
    "```python\n",
    "X_test.to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([Y_val, X_val], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([Y_train, X_train], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)\n",
    "```\n",
    "\n",
    "#### Step 3.2: Upload to S3\n",
    "\n",
    "- We are currently running inside of a SageMaker session\n",
    " - We can therefore use the object which represents this session to upload our data to the \"default\" S3 bucket. \n",
    "- Good practice: Provide a custom prefix (essentially an S3 folder) to make sure that you don't accidentally interfere with data uploaded from some other notebook or project. \n",
    "\n",
    "```python\n",
    "prefix = \"boston-xgboost-HL\"\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "```\n",
    "\n",
    "### Step 4: Train the XGBoost model\n",
    "\n",
    "What are the next steps?\n",
    "\n",
    "- Training and validation data are uploaded to S3\n",
    "- Construct XGBoost model and train it. \n",
    "- Make use of *High Level SageMaker API*.\n",
    " - High Level: Easier to read, less flexible\n",
    "\n",
    "How to construct an estimator? \n",
    "\n",
    "- The object we wish to train requires a location of a container with the training code.\n",
    "- For built in algorithms: Container is provided by Amazon.\n",
    "- Full name of the container is lengthy and depends on the region that we are operating in. \n",
    " - Use `get_image_uri` from `sagemaker.amazon.amazon_estimator` that constructs the image name for us. \n",
    " \n",
    "How to use `get_image_uri`? \n",
    "\n",
    "- Provide it with \n",
    " - our current region, obtained from session object: `sagemaker.Session()`\n",
    " - Name of the algorithm. The algorithm we will be using is XGBoost. Other available buil-in algorithms can be found in the list of [common parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)\n",
    " \n",
    "```python\n",
    "# Construct the image name for the training container.\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')\n",
    "```\n",
    "\n",
    "- Now that we know which container to use, we can construct the estimator object.\n",
    "- We will be using the [XGBoost](https://xgboost.readthedocs.io/en/latest/) algorithm.\n",
    " - Learn more about XGBoost starting with the docs which can be found at [https://xgboost.readthedocs.io/en/latest/]. \n",
    "\n",
    "How to construct the estimator object?\n",
    "\n",
    "```python\n",
    "# instantiate xgb object\n",
    "xgb = sagemaker.estimator.Estimator(container, \n",
    "                                    role,      \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge', \n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                                                   \n",
    "                                    sagemaker_session=session)\n",
    "```\n",
    "\n",
    "- `container` - Image name of the training container\n",
    "- `role`- The IAM role to use (our current role in this case)\n",
    "- `train_instance_count=1` - The number of instances to use for training\n",
    "- `train_instance_type='ml.m4.xlarge` - The type of instance to use for training\n",
    "- `output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix)` - Where to save the output (the model artifacts)\n",
    "-  `sagemaker_session=session` - The current SageMaker session\n",
    "\n",
    "Next: \n",
    "\n",
    "- Set hyperparameters\n",
    "- Additional information on the [XGBoost hyperparameter page](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)\n",
    "\n",
    "```python \n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=200)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Next: \n",
    "\n",
    "- It's time to train\n",
    "- Make sure SageMaker knows our input data is csv\n",
    "- Execute with the `.fit` method\n",
    "\n",
    "```python \n",
    "# wrapper around location, to make sure that SageMaker knows our data is in csv format.\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})\n",
    "```\n",
    "\n",
    "Remember the name of the notebook: `Boston Housing - XGBoost (Batch Transform) - High Level.ipynb` \n",
    "\n",
    "- **Batch Transform** - Method we will be using to test our model once we have trained it. Will be discussed later\n",
    "- **High Level** - Describes the API we will be using to get SAgeMaker to perform various machine learning tasks.\n",
    " - Referst to the Python SDK whose documentation can be found [here](https://sagemaker.readthedocs.io/en/latest/). This high level approach simplifies a lot of the details when working with SageMaker and can be very useful.\n",
    "\n",
    "Resources: \n",
    "\n",
    "- XGBoost - [List of XGBoost-based winning solutions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions) to a variety of competitions, at the linked XGBoost repository. \n",
    "- Etimators - Docs on [estimators](https://sagemaker.readthedocs.io/en/latest/estimators.html) for more information about this object. Essentially, the estimator is an object that specifies some details about how  a model will be trained. It gives the ability to create and deploy a model.\n",
    "- Training job - Used to train a specific estimator. \n",
    "\n",
    "When a training job is reqeuested you need to provide a few items:\n",
    " \n",
    "1. Location on S3 where training and validation data is procided\n",
    "2. Location on S3 where resulting model will be store (data is called model artifcasts)\n",
    "3. Location of a docker container (certainly the case if using a built in algorithm) to be used for training\n",
    "4. A description of the compute instance that should be used.\n",
    "\n",
    "SageMaker will execute the instance, load up the necessary docker container and execute it, pass in the location of the training data. After training, the model artifacts are packaged up and stored on S3. \n",
    "\n",
    "Resources: \n",
    "\n",
    "- High-level example of KMeans estimator [here](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model-create-training-job.html)\n",
    "- [XGBoost paper](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5bfdf09f_xgboost/xgboost.pdf)\n",
    "\n",
    "### Step 5: Test the model \n",
    "\n",
    "- Model has been fitted to the training data\n",
    "- Use validation data\n",
    "- Use SageMaker's batch transform functionality.\n",
    " - Start by building a transformer object from our fitted model\n",
    "- More about the transform and wait functions: [Transformer documentiation](https://sagemaker.readthedocs.io/en/latest/transformer.html)\n",
    "\n",
    "```python\n",
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "```\n",
    "- Begin a batch transform job\n",
    " - using trained model \n",
    " - apply it to the test data stored in S3. \n",
    " - Provide SageMaker with the type of used data\n",
    " - Make sure to let SageMaker know how to split our data up into chunks if data happens to be too large\n",
    "- `transform` function takes in the location of some test data, and some information about how that test data is formatted. \n",
    "\n",
    "```python\n",
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')\n",
    "```\n",
    "\n",
    "- batch transform job will be executed in the background\n",
    "- Since we need to wait for the results before continuing, we'll use the `wait()` method. \n",
    "\n",
    "```python\n",
    "xgb_transformer.wait()\n",
    "```\n",
    "- resulting output is stored on S3. \n",
    "- copy the output file from its S3 location and save it locally so we can use it. \n",
    "\n",
    "```\n",
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir\n",
    "```\n",
    "\n",
    "- Create scatter plot between the predicted and actual values. \n",
    "\n",
    "```python\n",
    "Y_pred = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "\n",
    "plt.scatter(Y_test, Y_pred)\n",
    "plt.xlabel(\"Median Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Median Price vs Predicted Price\")\n",
    "```\n",
    "\n",
    "<img src=\"../images/scatter1.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Clean up\n",
    "\n",
    "- disk space can be filled up\n",
    "- good idea to remove the files that were created along the way\n",
    "- can be done from terminal or notebook hub or with the following code\n",
    "\n",
    "```python \n",
    "# First we will remove all of the files contained in the data_dir directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# And then we delete the directory itself\n",
    "!rmdir $data_dir\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[NbConvertApp] Converting notebook SageMaker - 01 - Building a Model.ipynb to html',\n",
       " '[NbConvertApp] Writing 286077 bytes to SageMaker - 01 - Building a Model.html']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!jupyter nbconvert \"SageMaker - 01 - Building a Model\".ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
