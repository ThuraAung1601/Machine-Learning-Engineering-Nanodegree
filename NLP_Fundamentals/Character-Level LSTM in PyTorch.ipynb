{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level RNN\n",
    "\n",
    "In character-wise RNN problems the goal is to input characters and output characters which are most likely how the text proceeds. Individual characters are passed in to the level using one-hot-encoding. \n",
    "\n",
    "> One-hot-encoded characters: Every character is a binary feature 0/1, i.e. character is there vw. character not there.\n",
    "\n",
    "These one-hot-encoded features constitute the input layer. \n",
    "\n",
    "The input layer is passed to one (or more) hidden layers. The hidden layer(s) use LSTM cells. \n",
    "\n",
    "> An LSTM cell takes in the input as well as a hidden state. The hidden state is passed to the next cell. Simultaneously the cell produces an output. \n",
    "\n",
    "The last layer is the output layer. The activation function is a softmax as we'd like to get a probability distribution for the next character. This answers the question: Which character is most likely the next one in the sequence?\n",
    "\n",
    "#### How to get the batches right?\n",
    "\n",
    "How does batching works for RNN?\n",
    "\n",
    "Using RNNs, we're ttraining on sequences of data. Splitting these sequences into multiple shorter ones  enables us to take advantage of **matrix operations to make training more efficient.**\n",
    "\n",
    "RNNs are usually trained on multiple sequences in parallel. \n",
    "\n",
    "Example sequence: \n",
    "\n",
    "```\n",
    "# starting sequence\n",
    "[ 1 2 3 4 5 6 7 8 9 10 11 12 ]\n",
    "\n",
    "# split in two sequences, i.e. batch_size=2\n",
    "[ 1 2 3 4 5 6 ]\n",
    "[ 7 8 9 10 11 12 ]\n",
    "\n",
    "# Along batch size, we also choose seq_length\n",
    "# seq_length = 3, batch_size = 2\n",
    "\n",
    "Batch 1: [1 2 3 ] ---> Batch 2: [4 5 6 ]\n",
    "         [7 8 9 ]               [10 11 12 ]\n",
    "```\n",
    "\n",
    "The hidden state from batch 1 is transferred to batch 2. In this way, the sequence information is transferred across batches for each mini sequence.\n",
    "\n",
    "---\n",
    "\n",
    "# Character-Level LSTM in PyTorch\n",
    "\n",
    "- Construct a character-level LSTM in PyTorch\n",
    "- Network will train character by character on some text, then generate new text character by character. \n",
    "- Here, trained on the beginning of the book *Anna Karenina*\n",
    "- This model will be able to generate new text based on the text from the book\n",
    "\n",
    "This network is based on: \n",
    "\n",
    "- Andrej Karpathy's blog post: [The unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- Implementation in Torch: [Github link](https://github.com/karpathy/char-rnn)\n",
    "\n",
    "The general architecture of character-wise RNN:\n",
    "\n",
    "<img src=\"../images/charseq.jpeg\">\n",
    "\n",
    "#### Load required resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import numpy as np\n",
    "\n",
    "# time\n",
    "import time\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data\n",
    "\n",
    "- Load Anna Karenina text file\n",
    "- Convert it into integers for our network to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length (characters): 1985223\n",
      "First 100 characters:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/anna.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(\"Text length (characters):\", len(text))\n",
    "print(\"First 100 characters:\\n\")\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "- Creating a couple of dictionaries\n",
    " - These dictionaries are used to conver the characters to and from integers\n",
    "- Encoding the characters as integers makes it easier to use as input in the network.\n",
    "\n",
    "We create two dictionaries:\n",
    "\n",
    "1. `int2char`: maps integers to characters\n",
    "2. `char2int`: maps characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character 'i' is mapped to: 42\n",
      "Time for-loop: 0.8123791217803955\n",
      "Time list comprehension: 0.40152788162231445\n",
      "Length of encoded: 1985223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([46, 17, 39, 56, 38, 25, 50, 60, 48, 61])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map each character to an integer\n",
    "chars = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "### map text characters to int\n",
    "\n",
    "# example of how this mapping works\n",
    "print(\"Character 'i' is mapped to:\", char2int[\"i\"])\n",
    "\n",
    "# use for loop\n",
    "start = time.time()\n",
    "\n",
    "encoded = []\n",
    "for char in text:\n",
    "    encoded.append(char2int[char])\n",
    "\n",
    "encoded = np.array(encoded)\n",
    "end = time.time()\n",
    "print(\"Time for-loop:\", end-start)\n",
    "\n",
    "# use list comprehension\n",
    "start = time.time()\n",
    "encoded = np.array([char2int[char] for char in text])\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time list comprehension:\", end-start)\n",
    "print(\"Length of encoded:\", len(encoded))\n",
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing the data\n",
    "\n",
    "- LSTM expects input that is one-hot encoded\n",
    " - Each character is converted into an integer (using the dictionary),\n",
    " - then converted into a column vector. \n",
    " - This column vector will have the value of 1 and the rest will be filled with 0s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize arrays with zeros\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# check the function\n",
    "test_seq = np.array([[1,2,3]])\n",
    "one_hot_test = one_hot_encode(test_seq, 4)\n",
    "print(one_hot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making training mini-batches\n",
    "\n",
    "- We want to create mini-batches for training\n",
    "- We want our batches to be multiple sequences of some desired number of sequence steps. \n",
    "\n",
    "<img src=\"../images/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "- We'll take the encoded characters (passed in as the `arr` parameter)\n",
    "- split them into multiple sequences, given by `batch_size`. \n",
    "- Each sequence will be of length `seq_length`. \n",
    "\n",
    "## How to create batches?\n",
    "\n",
    "- We have 1,985,223 characters\n",
    "- N `(batch_size)`: Number of sequences in a batch\n",
    "- M `(seq_length)`: or number of time steps in a sequence\n",
    "- K: Total number of batches that we can make from the array `arr` (=total characters?)\n",
    "- Each batch contains $N \\times M$ characters, i.e. `batch_size * seq_length`\n",
    "- $N * M * K$ is the total number of characters to keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Discard text so we only have full mini-batches\n",
    "\n",
    "- What is a \"good\" `batch_size` and `seq_length`? \n",
    "- Decide and keep number of total characters $N * M * K$\n",
    "\n",
    "#### 2. Split `arr` into $N$ batches\n",
    "\n",
    "This can be done using `arr.reshape(size)` where size is a tuple containing the dimensions sized of the reshaped array. \n",
    "\n",
    "- We want N (`batch_size`) sequences in a batch\n",
    " - N can be determined as first dimension. \n",
    " - Second dimension can be set as `-1` as a placeholder in the size. It'll be fitted according to the data\n",
    "\n",
    "This yields an array that is $N \\times (M * K)$\n",
    "\n",
    "#### 3. Now iterate though array to get mini-batches. \n",
    "\n",
    "- Every batch is a $N \\times M$ window on the array\n",
    "- For each batch, the window moves over by `seq_length`. \n",
    "- In addition, we want to create input and target arrays\n",
    " - Targets are inputs shifted over by one character\n",
    "\n",
    "Recommendation: \n",
    "\n",
    "- Use `range`to take steps of size `n_steps` from 0 to `arr.shape[1]`, the total number of tokens in each sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_total: 30, seq_length: 10\n",
      "n_batches: 5\n",
      "my_arr.size before cutting: 154\n",
      "my_arr.size after cutting: 150\n",
      "my_arr.shape: (150,)\n",
      "my_arr.shape: (5, 30)\n",
      "0\n",
      "[[  0   1   2   3   4   5   6   7   8   9]\n",
      " [ 30  31  32  33  34  35  36  37  38  39]\n",
      " [ 60  61  62  63  64  65  66  67  68  69]\n",
      " [ 90  91  92  93  94  95  96  97  98  99]\n",
      " [120 121 122 123 124 125 126 127 128 129]]\n",
      "10\n",
      "[[ 10  11  12  13  14  15  16  17  18  19]\n",
      " [ 40  41  42  43  44  45  46  47  48  49]\n",
      " [ 70  71  72  73  74  75  76  77  78  79]\n",
      " [100 101 102 103 104 105 106 107 108 109]\n",
      " [130 131 132 133 134 135 136 137 138 139]]\n",
      "20\n",
      "[[ 20  21  22  23  24  25  26  27  28  29]\n",
      " [ 50  51  52  53  54  55  56  57  58  59]\n",
      " [ 80  81  82  83  84  85  86  87  88  89]\n",
      " [110 111 112 113 114 115 116 117 118 119]\n",
      " [140 141 142 143 144 145 146 147 148 149]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3 # N\n",
    "seq_length = 10 # M \n",
    "n_batches = 5\n",
    "\n",
    "# create data \n",
    "my_arr = np.arange(batch_size * seq_length * n_batches + 4)\n",
    "\n",
    "### function prep ###\n",
    "\n",
    "# calculate batch_size_total\n",
    "batch_size_total = batch_size * seq_length\n",
    "print(\"batch_size_total: {}, seq_length: {}\".format(batch_size_total, seq_length))\n",
    "\n",
    "# calculate number of batches\n",
    "n_batches = len(my_arr) // batch_size_total # K\n",
    "print(\"n_batches:\", n_batches)\n",
    "\n",
    "# Keep only enough chars to make full batches \n",
    "print(\"my_arr.size before cutting:\", my_arr.size)\n",
    "my_arr = my_arr[:n_batches * batch_size_total]\n",
    "print(\"my_arr.size after cutting:\", my_arr.size)\n",
    "print(\"my_arr.shape:\", my_arr.shape)\n",
    "\n",
    "# reshape with batches as rows\n",
    "my_arr = my_arr.reshape(n_batches, -1)\n",
    "print(\"my_arr.shape:\", my_arr.shape)\n",
    "\n",
    "for n in range(0, my_arr.shape[1], seq_length):\n",
    "    print(n)\n",
    "    print(my_arr[:, n:n+seq_length])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[46 17 39 56 38 25 50 60 48 61]\n",
      " [55 44 45 60 38 17 39 38 60 39]\n",
      " [25 45 62 60 44 50 60 39 60 22]\n",
      " [55 60 38 17 25 60 23 17 42 25]\n",
      " [60 55 39 12 60 17 25 50 60 38]\n",
      " [23 57 55 55 42 44 45 60 39 45]\n",
      " [60 21 45 45 39 60 17 39 62 60]\n",
      " [15 10  9 44 45 55 59  7 70 60]]\n",
      "\n",
      "y\n",
      " [[17 39 56 38 25 50 60 48 61 61]\n",
      " [44 45 60 38 17 39 38 60 39 38]\n",
      " [45 62 60 44 50 60 39 60 22 44]\n",
      " [60 38 17 25 60 23 17 42 25 22]\n",
      " [55 39 12 60 17 25 50 60 38 25]\n",
      " [57 55 55 42 44 45 60 39 45 62]\n",
      " [21 45 45 39 60 17 39 62 60 55]\n",
      " [10  9 44 45 55 59  7 70 60 64]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- The y sequence is shifted by one character!\n",
    "\n",
    "## Defining the network in PyTorch\n",
    "\n",
    "Next step is to implement the network in PyTorch. \n",
    "\n",
    "<img src=\"../images/charRNN.png\" width=500px>\n",
    "\n",
    "#### Model Structure\n",
    "\n",
    "Suggested structure in `__init__` :\n",
    "\n",
    "- Create and store the dictionaries\n",
    "- Define LSTM layer that takes in \n",
    " - `input_size` (number of characters), \n",
    " - hidden layer size: `n_hidden`\n",
    " - number of layers: `n_layers`\n",
    " - a dropout probability: `drop_prop`\n",
    "- Define fully-connected layer with parameters\n",
    " - input_size `n_hidden`  \n",
    " - `output_size`: number of characters\n",
    "- Finally, initialize the weights\n",
    "\n",
    "## LSTM Inputs/Outputs\n",
    "\n",
    "Basic LSTM layer: [Link](https://pytorch.org/docs/stable/nn.html#lstm)\n",
    "\n",
    "```python\n",
    "# Example\n",
    "self.lstm = nn.LSTM(input_size, \n",
    "                    n_hidden, \n",
    "                    n_layers, \n",
    "                    dropout=drop_prob,\n",
    "                    batch_first=True)\n",
    "```\n",
    "\n",
    "- `input_size`: Number of characters this cell expects as sequential input\n",
    "- `n_hidden`: Number of units in the hidden layers\n",
    "- forward function: stack up LSTM cells into layers using `.view`. \n",
    "\n",
    "Also required to create an initial hidden state of all zeros:\n",
    "\n",
    "```python\n",
    "self.init_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CharRNN class\n",
    "class CharRNN(nn.Module): \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # Define the LSTM\n",
    "        # Batch_first changes order for inputs!\n",
    "        self.lstm = nn.LSTM(len(self.chars), \n",
    "                            n_hidden, \n",
    "                            n_layers, \n",
    "                            dropout=drop_prob, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # Fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    # Define forward method\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\" \n",
    "        Forward pass through the network. \n",
    "        Inputs: x, hidden/cell state. \n",
    "        \"\"\"\n",
    "                \n",
    "        # Outputs, new hidden state from lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # Pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # contiguous to reshape output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## Put x through fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Method that initializes hidden state\n",
    "        Call at the beginning \n",
    "        \"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Writing a function so we have better control over number of epochs, learning rate and other parameters. \n",
    "\n",
    "- Adam optimizer\n",
    "- Cross-entropy loss\n",
    "\n",
    "Within the batch loop: \n",
    "\n",
    "- Detach hidden state from its history\n",
    "- Use [clip_grad_norm](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) to prevent gradients from exploding\n",
    " - [gradient clipping](https://deepai.org/machine-learning-glossary-and-terms/gradient-clipping)\n",
    " - Function clips gradient norm of an iterable of parameters. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Function to train the network\n",
    "        \n",
    "        Inputs\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch, timesteps\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    '''\n",
    "    # train mode on\n",
    "    net.train()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # Cross-Entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    # Check for gpu\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    # initialize counter\n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    # training loop\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # initialize hidden state with init_hidden method\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        # batch loop\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data \n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            \n",
    "            # Make them Torch tensors\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            # Send to gpu if applicable\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                \n",
    "                # \n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                # train mode on after validation\n",
    "                net.train()\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating and training the model\n",
    "\n",
    "- Create an instance of the CharRNN network\n",
    " - define hyperparameters\n",
    "- Define mini-batches size\n",
    "- start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate CharRNN class\n",
    "n_hidden = 256 # 512\n",
    "n_layers = 2 \n",
    "\n",
    "net = CharRNN(tokens=chars, \n",
    "              n_hidden = n_hidden, \n",
    "              n_layers = 2,\n",
    "              drop_prob=0.5, \n",
    "              lr = 0.001)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 10... Loss: 3.1237... Val Loss: 3.1249\n",
      "Epoch: 1/1... Step: 20... Loss: 3.4172... Val Loss: 3.4152\n",
      "Epoch: 1/1... Step: 30... Loss: 3.1932... Val Loss: 3.2921\n",
      "Epoch: 1/1... Step: 40... Loss: 3.1119... Val Loss: 3.1488\n",
      "Epoch: 1/1... Step: 50... Loss: 3.1418... Val Loss: 3.1239\n",
      "Epoch: 1/1... Step: 60... Loss: 3.1149... Val Loss: 3.1096\n",
      "Epoch: 1/1... Step: 70... Loss: 3.0897... Val Loss: 3.0551\n",
      "Epoch: 1/1... Step: 80... Loss: 3.0201... Val Loss: 2.9858\n",
      "Epoch: 1/1... Step: 90... Loss: 2.9510... Val Loss: 2.9470\n",
      "Epoch: 1/1... Step: 100... Loss: 2.9281... Val Loss: 2.8912\n",
      "Epoch: 1/1... Step: 110... Loss: 2.8835... Val Loss: 2.8221\n",
      "Epoch: 1/1... Step: 120... Loss: 2.8082... Val Loss: 2.7513\n",
      "Epoch: 1/1... Step: 130... Loss: 2.7148... Val Loss: 2.6715\n",
      "Epoch: 1/1... Step: 140... Loss: 2.6953... Val Loss: 2.6104\n",
      "Epoch: 1/1... Step: 150... Loss: 2.6770... Val Loss: 2.5547\n",
      "Epoch: 1/1... Step: 160... Loss: 2.5842... Val Loss: 2.5128\n",
      "Epoch: 1/1... Step: 170... Loss: 2.5460... Val Loss: 2.4810\n",
      "Epoch: 1/1... Step: 180... Loss: 2.4858... Val Loss: 2.4555\n",
      "Epoch: 1/1... Step: 190... Loss: 2.4780... Val Loss: 2.4361\n",
      "Epoch: 1/1... Step: 200... Loss: 2.4787... Val Loss: 2.3921\n",
      "Epoch: 1/1... Step: 210... Loss: 2.4112... Val Loss: 2.4856\n",
      "Epoch: 1/1... Step: 220... Loss: 2.5770... Val Loss: 2.3846\n",
      "Epoch: 1/1... Step: 230... Loss: 2.4734... Val Loss: 2.3444\n",
      "Epoch: 1/1... Step: 240... Loss: 2.3989... Val Loss: 2.3151\n",
      "Epoch: 1/1... Step: 250... Loss: 2.3450... Val Loss: 2.2881\n",
      "Epoch: 1/1... Step: 260... Loss: 2.3147... Val Loss: 2.2603\n",
      "Epoch: 1/1... Step: 270... Loss: 2.3040... Val Loss: 2.2462\n",
      "Epoch: 1/1... Step: 280... Loss: 2.2978... Val Loss: 2.2226\n",
      "Epoch: 1/1... Step: 290... Loss: 2.2522... Val Loss: 2.2018\n",
      "Epoch: 1/1... Step: 300... Loss: 2.1897... Val Loss: 2.1808\n",
      "Epoch: 1/1... Step: 310... Loss: 2.1700... Val Loss: 2.1638\n",
      "Epoch: 1/1... Step: 320... Loss: 2.1940... Val Loss: 2.1533\n",
      "Epoch: 1/1... Step: 330... Loss: 2.1942... Val Loss: 2.1417\n",
      "Epoch: 1/1... Step: 340... Loss: 2.2145... Val Loss: 2.1259\n",
      "Epoch: 1/1... Step: 350... Loss: 2.1642... Val Loss: 2.1150\n",
      "Epoch: 1/1... Step: 360... Loss: 2.1610... Val Loss: 2.0958\n",
      "Epoch: 1/1... Step: 370... Loss: 2.1149... Val Loss: 2.0848\n",
      "Epoch: 1/1... Step: 380... Loss: 2.1233... Val Loss: 2.0717\n",
      "Epoch: 1/1... Step: 390... Loss: 2.1788... Val Loss: 2.0606\n",
      "Epoch: 1/1... Step: 400... Loss: 2.1436... Val Loss: 2.0488\n",
      "Epoch: 1/1... Step: 410... Loss: 2.0948... Val Loss: 2.0399\n",
      "Epoch: 1/1... Step: 420... Loss: 2.1707... Val Loss: 2.0361\n",
      "Epoch: 1/1... Step: 430... Loss: 2.0419... Val Loss: 2.0219\n",
      "Epoch: 1/1... Step: 440... Loss: 2.1164... Val Loss: 2.0088\n",
      "Epoch: 1/1... Step: 450... Loss: 2.0703... Val Loss: 2.0037\n",
      "Epoch: 1/1... Step: 460... Loss: 2.0581... Val Loss: 1.9862\n",
      "Epoch: 1/1... Step: 470... Loss: 2.0368... Val Loss: 1.9785\n",
      "Epoch: 1/1... Step: 480... Loss: 2.0509... Val Loss: 1.9735\n",
      "Epoch: 1/1... Step: 490... Loss: 2.1091... Val Loss: 1.9601\n",
      "Epoch: 1/1... Step: 500... Loss: 1.9766... Val Loss: 1.9496\n",
      "Epoch: 1/1... Step: 510... Loss: 2.0420... Val Loss: 1.9494\n",
      "Epoch: 1/1... Step: 520... Loss: 2.0233... Val Loss: 1.9389\n",
      "Epoch: 1/1... Step: 530... Loss: 1.9863... Val Loss: 1.9212\n",
      "Epoch: 1/1... Step: 540... Loss: 1.9579... Val Loss: 1.9187\n",
      "Epoch: 1/1... Step: 550... Loss: 1.9660... Val Loss: 1.9136\n",
      "Epoch: 1/1... Step: 560... Loss: 1.9605... Val Loss: 1.9003\n",
      "Epoch: 1/1... Step: 570... Loss: 1.9104... Val Loss: 1.8923\n",
      "Epoch: 1/1... Step: 580... Loss: 1.9305... Val Loss: 1.8865\n",
      "Epoch: 1/1... Step: 590... Loss: 1.9578... Val Loss: 1.8862\n",
      "Epoch: 1/1... Step: 600... Loss: 1.8513... Val Loss: 1.8771\n",
      "Epoch: 1/1... Step: 610... Loss: 1.9291... Val Loss: 1.8682\n",
      "Epoch: 1/1... Step: 620... Loss: 1.9748... Val Loss: 1.8572\n",
      "Epoch: 1/1... Step: 630... Loss: 1.8770... Val Loss: 1.8502\n",
      "Epoch: 1/1... Step: 640... Loss: 1.8212... Val Loss: 1.8479\n",
      "Epoch: 1/1... Step: 650... Loss: 1.8545... Val Loss: 1.8445\n",
      "Epoch: 1/1... Step: 660... Loss: 1.9493... Val Loss: 1.8434\n",
      "Epoch: 1/1... Step: 670... Loss: 1.8227... Val Loss: 1.8294\n",
      "Epoch: 1/1... Step: 680... Loss: 1.8829... Val Loss: 1.8273\n",
      "Epoch: 1/1... Step: 690... Loss: 1.9569... Val Loss: 1.8240\n",
      "Epoch: 1/1... Step: 700... Loss: 1.8725... Val Loss: 1.8191\n",
      "Epoch: 1/1... Step: 710... Loss: 1.9365... Val Loss: 1.8056\n",
      "Epoch: 1/1... Step: 720... Loss: 1.8573... Val Loss: 1.8041\n",
      "Epoch: 1/1... Step: 730... Loss: 1.7872... Val Loss: 1.8001\n",
      "Epoch: 1/1... Step: 740... Loss: 1.8773... Val Loss: 1.7922\n",
      "Epoch: 1/1... Step: 750... Loss: 1.8878... Val Loss: 1.7876\n",
      "Epoch: 1/1... Step: 760... Loss: 1.7684... Val Loss: 1.7785\n",
      "Epoch: 1/1... Step: 770... Loss: 1.7910... Val Loss: 1.7808\n",
      "Epoch: 1/1... Step: 780... Loss: 1.8050... Val Loss: 1.7750\n",
      "Epoch: 1/1... Step: 790... Loss: 1.7898... Val Loss: 1.7664\n",
      "Epoch: 1/1... Step: 800... Loss: 1.8556... Val Loss: 1.7636\n",
      "Epoch: 1/1... Step: 810... Loss: 1.7571... Val Loss: 1.7576\n",
      "Epoch: 1/1... Step: 820... Loss: 1.7985... Val Loss: 1.7543\n",
      "Epoch: 1/1... Step: 830... Loss: 1.7740... Val Loss: 1.7467\n",
      "Epoch: 1/1... Step: 840... Loss: 1.8128... Val Loss: 1.7471\n",
      "Epoch: 1/1... Step: 850... Loss: 1.7959... Val Loss: 1.7369\n",
      "Epoch: 1/1... Step: 860... Loss: 1.8648... Val Loss: 1.7344\n",
      "Epoch: 1/1... Step: 870... Loss: 1.7425... Val Loss: 1.7326\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # 128\n",
    "seq_length = 32 # 100\n",
    "n_epochs = 1 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.01, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to improve performance?\n",
    "\n",
    "- Change parameters based on training and validation loss\n",
    "- Training loss much lower than validation? Overfitting!\n",
    " - Increase regularization (more dropout)\n",
    " - Use a smaller network\n",
    "- Training and validation losses close? Probably underfitting\n",
    " - Increase the size of the network\n",
    " \n",
    "#### Hyperparameters\n",
    "\n",
    "Model: \n",
    "\n",
    "- `n_hidden` - Number of units in the hidden layers\n",
    "- `n_layers` - Number of hidden LSTM layers to use.\n",
    "\n",
    "Training: \n",
    "\n",
    "- `batch_size` - Number of sequences running through the network in one pass\n",
    "- `seq_length` - Number of characters in the sequence the network is trained on. \n",
    "- `lr` - learning rate for training\n",
    "\n",
    "## Tips and Tricks (Karpathy)\n",
    "\n",
    "- [Link](https://github.com/karpathy/char-rnn#tips-and-tricks)\n",
    "\n",
    "#### Monitoring Validation Loss vs. Training Loss\n",
    "\n",
    "- Keep track of the difference between training loss and validation loss\n",
    "- Training loss much lower than validation loss? Network might be **overfitting**.\n",
    " - Solutions: Decrease network size or increase dropout\n",
    "- Training/validation loss about equal? Then Model is underfitting.\n",
    " - Increase the size of your model (layers or neurons per layer)\n",
    " \n",
    "#### Approximate number of parameters\n",
    "\n",
    "- `n_layers` of 2 or 3 is advised\n",
    "- `n_hidden` can be adjusted based on how much data are available. \n",
    "\n",
    "#### Best models strategy\n",
    "\n",
    "- Be uncomfortable on making the network larger\n",
    "- Try different dropout values\n",
    "- Whatever model has the best validation performance is the one you should use in the end\n",
    "\n",
    "Note that it is common practice to run many different models with many different hyperparameter setting, and in the end take whatever checkpoint gave the best validation performance. \n",
    "\n",
    "- In addition, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative. \n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "After training, save the mdoel so it can be loaded it in again if we need to. \n",
    "\n",
    "- Save parameters needed to create the same architecture, hidden layer hyperparameters and text characters.c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rnn_1_epoch.net\"\n",
    "\n",
    "# save inputs to the model\n",
    "checkpoint = {\"n_hidden\": net.n_hidden,\n",
    "              \"n_layers\": net.n_layers,\n",
    "              \"state_dict\": net.state_dict(),\n",
    "              \"tokens\": net.chars}\n",
    "\n",
    "with open(model_name, \"wb\") as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Model is trained, how to make predictions about the next characters? \n",
    "\n",
    "- Pass in a character and have the network to predict the next character.\n",
    "- Take this character, pass it back in, get another predicted character. \n",
    "- Keep doing this as long as you want...\n",
    "\n",
    "#### Note on the predict function\n",
    "\n",
    "- Output of the RNN is from a fully-connected layer and it outputs a **distribution of next character scores** (softmax)\n",
    "- Take the character with the highest probability!\n",
    "\n",
    "#### Top K Sampling\n",
    "\n",
    "- Predictions are from a categorical probability distribution over all the possible characters (tokens)\n",
    "- Only consider some $K$ most probable characters\n",
    "- Use [topk](https://pytorch.org/docs/stable/torch.html#torch.topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before squeeze: (1, 2, 3)\n",
      "Now squeeze it:\n",
      " [[2 2 2]\n",
      " [2 2 2]]\n",
      "Shape after squeeze (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# numpy squeeze \n",
    "in_arr = np.array([[[2, 2, 2],\n",
    "                   [2, 2, 2]]])\n",
    "\n",
    "print(\"Shape before squeeze:\", in_arr.shape)\n",
    "\n",
    "# Remove single-dimensional enries from shapae of an array\n",
    "print(\"Now squeeze it:\\n\", in_arr.squeeze())\n",
    "print(\"Shape after squeeze\", in_arr.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k = None):\n",
    "    \"\"\"\n",
    "    Given a character, predict next character\n",
    "    Returns predicted character and hidden state\n",
    "    \"\"\"\n",
    "    \n",
    "    # inputs as tensor\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encode(x, len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "    \n",
    "    # check if gpu\n",
    "    if(train_on_gpu):\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "    \n",
    "    # get character probs\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    \n",
    "    # move to cpu\n",
    "    if(train_on_gpu):\n",
    "        p = p.cpu() \n",
    "        \n",
    "    # get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "    # Convert p to numpy, remove single-dimensional entries\n",
    "    p = p.numpy().squeeze()\n",
    "    \n",
    "    # Select the likely next character\n",
    "    # ...with some element of randomness\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "    \n",
    "    # return encoded value of the predicted char\n",
    "    # ...as well as the hidden state\n",
    "    return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Priming and generating text\n",
    "\n",
    "- Prime the network to build up a hidden state. Otherwise network start out generating characters at random.\n",
    "- In general, first bunch of characters will be little rough since it hasnt built up a long history of characters to predict from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna, when the mome her\n",
      "was his charted a dear over that she and to his she shilt, the hound on\n",
      "the was how, but tood a comen of stopan, he was a call how, whither the wifed how\n",
      "tern that and a dinter as in the\n",
      "head to a she shall of the daid\n",
      "her with him a said on their she would how at her trun is\n",
      "her \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 300, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said all think of the will, but tome than his tree he sould his shird that when she sorting in a still the dealt of ans her,\n",
      "than intorest tank the his and soman the came time\n",
      "seast that with and that a stopt of a serition in anst the compontather.\n",
      "\n",
      "Afting on sered, and always with the werl was all his hir a little, but his but\n",
      "and his to herself all the\n",
      "say and the wart in its stret is thas he had betanter that, her, and\n",
      "had begon to her the cart, and to\n",
      "to the somet of it true, as he had seaning, hore which and trang in sick and\n",
      "strat the dids to at his\n",
      "she\n",
      "sond into her would she sure his\n",
      "hungand, and all him and\n",
      "tome all her strest one he came the was\n",
      "that ang alway, had thinked,\n",
      "and to be all shiming have have\n",
      "all had been that thould not him the conversated in a sand him to he\n",
      "could, though, but the herting as the\n",
      "harsed in the did that and she sand\n",
      "a seck a lade of in the wonding, with the horsian in a much stired to this with which the sort in\n",
      "herself, and his\n",
      "content, thought asted at and with the sair, with him to and him\n",
      "him had so than\n",
      "hersern and she working of imperessed is says him.\n",
      "\n",
      "\"Yele had not strong the rast,\n",
      "and strie in sorring. At her, and all a carrie intanting that to the content into a said that he wife a she with her senter. Alexandryone and her,\n",
      "but he was this still thas,\" said this sone the came had she\n",
      "had that was a looked the hurdly to and a position to say at her that a deart to step and his sair herself at a\n",
      "said in it was not so the\n",
      "dars, had not as tho shead to her so comile out that this an a sand and all the sompition at ang to her hand him that his she hearsess have, women when a stand him.\n",
      "\n",
      "\"I said so\n",
      "his at a master a then to\n",
      "hims all the childrang to that who all he was not\n",
      "strien him.\n",
      "\n",
      "\"Are she how that with his mand, ald she said to had see so she she sorratele, thought that he hord the might to hills him befile that he and the massing\n",
      "of a\n",
      "said. That, tarling, who which that he have and the sturd of impritacal and a dead the \n"
     ]
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_1_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[NbConvertApp] Converting notebook Character-Level LSTM in PyTorch.ipynb to html',\n",
       " '[NbConvertApp] Writing 345659 bytes to Character-Level LSTM in PyTorch.html']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!jupyter nbconvert *.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
