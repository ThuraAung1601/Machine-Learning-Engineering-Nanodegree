{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "## NLP Pipelines\n",
    "\n",
    "NLP pipelines in general consist of three stages: \n",
    "\n",
    "- Text Processing\n",
    "- Feature Extraction\n",
    "- Modeling\n",
    "\n",
    "#### Text Processing \n",
    "\n",
    "- Source of text, such as html needs to prepared i.e. the text from the web needs to be extracted.\n",
    " - Example: [Kingfisher](https://en.wikipedia.org/wiki/Kingfisher)\n",
    "- Goal: Extract plain text\n",
    "\n",
    "Other processing text\n",
    "\n",
    "- Change capitalization (capitalization generally does not change meaning)\n",
    "- Punctuation\n",
    "- Common words which do not add meaning can often be filtered out\n",
    "- Remove endings of words, i.e. reduce to word stem.\n",
    "\n",
    "## Feature Extraction\n",
    "\n",
    "#### Bag of Words\n",
    "\n",
    "- Treats document as an un-ordered collection or bag of words. \n",
    "- Each observation consists of the words of the text as features. \n",
    "\n",
    "To obtain a bag of words we need to apply appropriate text processing steps. The resulting \"Tokens\" are then treated as an un-ordered collection or set. \n",
    "\n",
    "- Vectorize all words that were used. \n",
    "- Represent occurence using Document-Term Matrix\n",
    "- Compare different obersvations using this representation\n",
    " - Mathematical way to express similarities: Dot product the two row vectors. \n",
    " - Dot product is the sum of the products of corresponding elements. \n",
    "\n",
    "$$\n",
    "ab = \\sum a_0 b_0 + a_1 b_1 + ... + a_n b_n \n",
    "$$\n",
    "\n",
    "- Better measure: **Cosine similarity**\n",
    "\n",
    "$$ \n",
    "cos(\\Theta) = \\frac{a \\cdot b}{||a|| \\cdot ||b||}\n",
    "$$\n",
    "\n",
    "where $||a|| = \\sqrt{a_1^2 + a_2^2 + ... + a_n^2}$ represents their magnitudes or Euclidian norms.\n",
    "\n",
    "If you think of these arrows in some n-dimensional space, then this is equal to the cosine of the angle theta between them. \n",
    "\n",
    "- Identical vectors have cosine of 1\n",
    "- Orthogonal vectors have cosine of 0\n",
    "- Opposite vectors have cosine of -1\n",
    "\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "One limiation of Bag of words: Treats every word as being equally important. \n",
    "\n",
    "We can approach this by collecting each word's frequency and then dividing the term frequencies by the document frequency of that term to get a relative measure.\n",
    "\n",
    "Properties of this metric: \n",
    "- Proportional to the frequency of occurence of a term in a document. \n",
    "- Inversely proportional to the number of document it appears in.\n",
    "- Highlights the words that are more unique to a document\n",
    "\n",
    "> TF-IDF transform is simply the product of two words, \n",
    "\n",
    "\n",
    "$$\n",
    "tfidf(t, d, D) = tf(t, d) \\cdot idf(t, D),\n",
    "$$\n",
    "\n",
    "with $tf(t,d)$ as the term frequency and $idf(t, D)$ being the inverse document frequency.\n",
    "\n",
    "$$\n",
    "tf(t,d) = \\frac{count(t, d)}{|d|}\n",
    "$$\n",
    "\n",
    "is the raw count of a term t in a document d divided by the total number of terms in d.\n",
    "\n",
    "$$\n",
    "idf(t,D) = log \\Big( \\frac{|D|}{d \\in D: t\\in d} \\Big)\n",
    "$$\n",
    "\n",
    "The total number of Documents in the collection, divided by the number of documents where t is present. \n",
    "\n",
    "Several variations exist. \n",
    "\n",
    "#### One-Hot Encoding\n",
    "\n",
    "In the context of Language Processing One-hot encoding means\n",
    "\n",
    "> Treat each word like a class, assign it a vector. If the word is present this variable is one and zero otherwise (dummy variable).\n",
    "\n",
    "#### Word Embeddings\n",
    "\n",
    "Problem of One-hot encoding: May brea down when we have a large vocabulary to deal with\n",
    "\n",
    "- We need a way to control the size of our word representation by limiting it to a fixed-size vector. \n",
    "- We want to find an embeding for each word in some vector space and we want it to exhibit some desired properties. \n",
    " - I.e. If words are close in meaning they should be close to each other in the vector space compared to words that are not. \n",
    " - Also, if two pairs of words have a similar difference in their meanings, they should be approximately equally separated in the embedded space. \n",
    "\n",
    "#### Word2Vec\n",
    "\n",
    "Word2Vec is perhaps one of the most popular examples of word embeddings used in practice. It transforms word to vectors, just like suggested with the name.\n",
    "\n",
    "Core idea: \n",
    "\n",
    "> A model that is able to predict a word, given neighboring words, or vice versa, is likely to capture the contextual meanings of words very well. \n",
    "\n",
    "- Continous bag of words (CBoW) or Skip-gram\n",
    "\n",
    "Skip-gram: \n",
    "\n",
    "- Pick any word from a sentence, \n",
    "- convert it into a one-hot encoded vector and feed it into a neural network (or some ohter probabilistic model). \n",
    "- Train model to predict context words as best as it can. \n",
    "- Take an intermediate representation like a hidden layer in a neural network.\n",
    "- Outputs of that layer for a given word become the corresponding word vector.\n",
    "\n",
    "Properties Word2Vec:\n",
    "\n",
    "- Robust, distributed representation\n",
    "- Vector size independent of vocabulary.\n",
    "- Train once, store in lookup table. \n",
    "- Deep Learning ready!\n",
    "\n",
    "#### GloVe\n",
    "\n",
    "Global vectors for word representation is another approach\n",
    "\n",
    "- Probability that word j appears in the context of word i is computed\n",
    "- Count all occurences of i and j in our text collection and then normalize account to get a probability. \n",
    "- Using additional context and target vector\n",
    "- For any word ij, we want the dot product of their word vectors to be equal to their co-occurence probability. \n",
    "\n",
    "Paper: [GloVe (PDF)](https://www.aclweb.org/anthology/D14-1162)\n",
    "\n",
    "#### Embeddings for Deep Learning\n",
    "\n",
    "> **Distributional Hypothesis:** Words that occur in the same contexts tend to have similar meanings.\n",
    "\n",
    "Example: Which word are we searching?\n",
    "\n",
    "- Would you like a cup of ___ ? \n",
    "- I like my ___ black. \n",
    "- I need my morning ___ before I can do anything?\n",
    "\n",
    "The point here is that in these contexts tea and coffee are actually very similar. Therefore, when a large collection of sentences is used to learn in embedding,  words with common context words tend to get pulled closer and closer together. \n",
    "\n",
    "Adding different dimensions enable to capture similarities and differences in the same embedding. \n",
    "\n",
    "I.e. coffee and tea are similar in the dimension of being both beverages (one dimension) \n",
    "\n",
    "Pre-trained embedding can be used as a lookup if the application is not too specific. Using an embedding look up for NLP can be thought of just as using pretrained nets (e.g. Alexnet, BTG16) and only learn the later layers. \n",
    "\n",
    "## Modeling\n",
    "\n",
    "Final stage includes: \n",
    "\n",
    "- Designing a model (ML or statsistical one)\n",
    "- Fitting parametesr to training data using an optimization procedure\n",
    "- Using it to make predictions \n",
    "\n",
    "Because we vectorized the model we could in principle utilize pretty much any machine learning model we like."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
